{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "polar-muscle",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "\n",
    "- Python3.5-3.8\n",
    "- Tesnorflow (`pip install tensorflow`)\n",
    "- Tensorflow Dataset (`pip install tensorflow_datasets`)\n",
    "- Requsets (`pip install requests`)\n",
    "- Easytensor (`pip install easytensor`)\n",
    "- Matplotlib (`pip install matplotlib`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "instant-review",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /Users/kamalkamalaldin/virtualenv-3.8/lib/python3.8/site-packages (2.4.1)\n",
      "Requirement already satisfied: tensorflow_datasets in /Users/kamalkamalaldin/virtualenv-3.8/lib/python3.8/site-packages (4.2.0)\n",
      "Requirement already satisfied: requests in /Users/kamalkamalaldin/virtualenv-3.8/lib/python3.8/site-packages (2.25.1)\n",
      "Requirement already satisfied: easytensor in /Users/kamalkamalaldin/virtualenv-3.8/lib/python3.8/site-packages (0.0.3)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.3.4-cp38-cp38-macosx_10_9_x86_64.whl (8.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 8.5 MB 1.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pillow>=6.2.0\n",
      "  Downloading Pillow-8.1.2-cp38-cp38-macosx_10_10_x86_64.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 3.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.3.1-cp38-cp38-macosx_10_9_x86_64.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 346 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.15 in /Users/kamalkamalaldin/virtualenv-3.8/lib/python3.8/site-packages (from matplotlib) (1.19.5)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/kamalkamalaldin/virtualenv-3.8/lib/python3.8/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /Users/kamalkamalaldin/virtualenv-3.8/lib/python3.8/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: six in /Users/kamalkamalaldin/virtualenv-3.8/lib/python3.8/site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/kamalkamalaldin/virtualenv-3.8/lib/python3.8/site-packages (from requests) (1.26.3)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/kamalkamalaldin/virtualenv-3.8/lib/python3.8/site-packages (from requests) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kamalkamalaldin/virtualenv-3.8/lib/python3.8/site-packages (from requests) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/kamalkamalaldin/virtualenv-3.8/lib/python3.8/site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /Users/kamalkamalaldin/virtualenv-3.8/lib/python3.8/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: tensorboard~=2.4 in /Users/kamalkamalaldin/virtualenv-3.8/lib/python3.8/site-packages (from tensorflow) (2.4.1)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /Users/kamalkamalaldin/virtualenv-3.8/lib/python3.8/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /Users/kamalkamalaldin/virtualenv-3.8/lib/python3.8/site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /Users/kamalkamalaldin/virtualenv-3.8/lib/python3.8/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /Users/kamalkamalaldin/virtualenv-3.8/lib/python3.8/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: grpcio~=1.32.0 in /Users/kamalkamalaldin/virtualenv-3.8/lib/python3.8/site-packages (from tensorflow) (1.32.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /Users/kamalkamalaldin/virtualenv-3.8/lib/python3.8/site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /Users/kamalkamalaldin/virtualenv-3.8/lib/python3.8/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /Users/kamalkamalaldin/virtualenv-3.8/lib/python3.8/site-packages (from tensorflow) (0.36.2)\n",
      "Requirement already satisfied: absl-py~=0.10 in /Users/kamalkamalaldin/virtualenv-3.8/lib/python3.8/site-packages (from tensorflow) (0.10.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /Users/kamalkamalaldin/virtualenv-3.8/lib/python3.8/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /Users/kamalkamalaldin/virtualenv-3.8/lib/python3.8/site-packages (from tensorflow) (0.3.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /Users/kamalkamalaldin/virtualenv-3.8/lib/python3.8/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /Users/kamalkamalaldin/virtualenv-3.8/lib/python3.8/site-packages (from tensorflow) (3.15.4)\n",
      "Requirement already satisfied: h5py~=2.10.0 in /Users/kamalkamalaldin/virtualenv-3.8/lib/python3.8/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /Users/kamalkamalaldin/virtualenv-3.8/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow) (1.27.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/kamalkamalaldin/virtualenv-3.8/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/kamalkamalaldin/virtualenv-3.8/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow) (52.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/kamalkamalaldin/virtualenv-3.8/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/kamalkamalaldin/virtualenv-3.8/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/kamalkamalaldin/virtualenv-3.8/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/kamalkamalaldin/virtualenv-3.8/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/kamalkamalaldin/virtualenv-3.8/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Users/kamalkamalaldin/virtualenv-3.8/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.2.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/kamalkamalaldin/virtualenv-3.8/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/kamalkamalaldin/virtualenv-3.8/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/kamalkamalaldin/virtualenv-3.8/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: tqdm in /Users/kamalkamalaldin/virtualenv-3.8/lib/python3.8/site-packages (from tensorflow_datasets) (4.58.0)\n",
      "Requirement already satisfied: dill in /Users/kamalkamalaldin/virtualenv-3.8/lib/python3.8/site-packages (from tensorflow_datasets) (0.3.3)\n",
      "Requirement already satisfied: importlib-resources in /Users/kamalkamalaldin/virtualenv-3.8/lib/python3.8/site-packages (from tensorflow_datasets) (5.1.1)\n",
      "Requirement already satisfied: promise in /Users/kamalkamalaldin/virtualenv-3.8/lib/python3.8/site-packages (from tensorflow_datasets) (2.3)\n",
      "Requirement already satisfied: tensorflow-metadata in /Users/kamalkamalaldin/virtualenv-3.8/lib/python3.8/site-packages (from tensorflow_datasets) (0.28.0)\n",
      "Requirement already satisfied: future in /Users/kamalkamalaldin/virtualenv-3.8/lib/python3.8/site-packages (from tensorflow_datasets) (0.18.2)\n",
      "Requirement already satisfied: attrs>=18.1.0 in /Users/kamalkamalaldin/virtualenv-3.8/lib/python3.8/site-packages (from tensorflow_datasets) (20.3.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /Users/kamalkamalaldin/virtualenv-3.8/lib/python3.8/site-packages (from tensorflow-metadata->tensorflow_datasets) (1.53.0)\n",
      "Installing collected packages: pillow, kiwisolver, cycler, matplotlib\n",
      "Successfully installed cycler-0.10.0 kiwisolver-1.3.1 matplotlib-3.3.4 pillow-8.1.2\n"
     ]
    }
   ],
   "source": [
    "! pip install tensorflow tensorflow_datasets requests easytensor matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-patient",
   "metadata": {},
   "source": [
    "# Train a simple MNIST model\n",
    "Here, we will train a simple model that can classify english digits from 1 to 9. The MNIST dataset is made up of small images that are 28x28 pixels.\n",
    "\n",
    "The full explanation of how this model is trained can be found [here](https://www.tensorflow.org/datasets/keras_example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "guilty-procurement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'mnist',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "advised-bullet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_img(image, label):\n",
    "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "  return tf.cast(image, tf.float32) / 255., label\n",
    "\n",
    "ds_train = ds_train.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_train = ds_train.cache()\n",
    "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
    "ds_train = ds_train.batch(128)\n",
    "ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "social-taxation",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test = ds_test.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_test = ds_test.batch(128)\n",
    "ds_test = ds_test.cache()\n",
    "ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "elect-translator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "469/469 [==============================] - 3s 3ms/step - loss: 0.6090 - sparse_categorical_accuracy: 0.8377 - val_loss: 0.1966 - val_sparse_categorical_accuracy: 0.9434\n",
      "Epoch 2/6\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1811 - sparse_categorical_accuracy: 0.9487 - val_loss: 0.1393 - val_sparse_categorical_accuracy: 0.9596\n",
      "Epoch 3/6\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1239 - sparse_categorical_accuracy: 0.9654 - val_loss: 0.1164 - val_sparse_categorical_accuracy: 0.9657\n",
      "Epoch 4/6\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0898 - sparse_categorical_accuracy: 0.9732 - val_loss: 0.0936 - val_sparse_categorical_accuracy: 0.9718\n",
      "Epoch 5/6\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0768 - sparse_categorical_accuracy: 0.9785 - val_loss: 0.0835 - val_sparse_categorical_accuracy: 0.9750\n",
      "Epoch 6/6\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0600 - sparse_categorical_accuracy: 0.9833 - val_loss: 0.0805 - val_sparse_categorical_accuracy: 0.9745\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x16abe3850>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128,activation='relu'),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    ds_train,\n",
    "    epochs=6,\n",
    "    validation_data=ds_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "gothic-blocking",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export_path: /Users/kamalkamalaldin/repos/python-client/my_model\n",
      "INFO:tensorflow:Assets written to: /Users/kamalkamalaldin/repos/python-client/my_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/kamalkamalaldin/repos/python-client/my_model/assets\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "export_path = os.path.join(os.getcwd(), \"my_model\")\n",
    "print(\"export_path: {}\".format(export_path))\n",
    "\n",
    "tf.keras.models.save_model(\n",
    "    model,\n",
    "    export_path,\n",
    "    overwrite=True,\n",
    "    include_optimizer=True,\n",
    "    save_format=None,\n",
    "    signatures=None,\n",
    "    options=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graduate-romania",
   "metadata": {},
   "source": [
    "# Upload to EasyTensor\n",
    "Now that we have the model exported, we can upload it to cloud and have EasyTensor serve it for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "median-weight",
   "metadata": {},
   "outputs": [],
   "source": [
    "import easytensor\n",
    "model_id, access_token = easytensor.upload_model(\"Showing dana Model\", export_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "described-stuff",
   "metadata": {},
   "source": [
    "That's it! The model is now in the cloud with one line of code. Each model will have an ID and an access token. Anyone with the access token can send requests to the model, so make sure you store it somewhere safe!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "executed-welcome",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model ID: d38e2b13-b22c-4556-9509-2418fdc6a118\n",
      "access token: e3525e19-2ec9-4a1b-9fa1-48d35e3a2ba7\n"
     ]
    }
   ],
   "source": [
    "print(\"model ID:\", model_id)\n",
    "print(\"access token:\", access_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "involved-performance",
   "metadata": {},
   "source": [
    "# Use the model\n",
    "Now let's try to use the model via a simple http request. We will use a random image from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "formed-wales",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of image batches: 469\n",
      "Each batch contains 128 images\n",
      "Each image has shape (28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# We need to get the label and image in the same iterator to get around shuffling.\n",
    "image_label_tuples = list(map(lambda x: (x[0], x[1]), ds_train)) \n",
    "image_batches = list(map(lambda x: x[0], image_label_tuples)) # shape (x, 128, 28, 28, 1)\n",
    "label_batches = list(map(lambda x: x[1], image_label_tuples)) # shape (x, 128, 1)\n",
    "print(\"number of image batches:\", len(image_batches))\n",
    "print(\"Each batch contains\", image_batches[0].shape[0], \"images\")\n",
    "print(\"Each image has shape\", image_batches[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "close-circus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image to send:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure\n",
    "plt.imshow(image_batches[0][0], cmap='gray')\n",
    "print(\"Image to send:\")\n",
    "plt.show()\n",
    "\n",
    "image_to_predict = image_batches[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "controlled-bradley",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from server:\n",
      "{'predictions': [[10.6536636,\n",
      "                  -5.54453468,\n",
      "                  -2.0866,\n",
      "                  -2.84490108,\n",
      "                  -8.73831654,\n",
      "                  -1.36901224,\n",
      "                  -0.546078086,\n",
      "                  -2.26826048,\n",
      "                  -3.7583766,\n",
      "                  -4.20979548]]}\n",
      "Digit prediction:  [0]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "response = requests.post(\n",
    "    \"https://app.easytensor.com/query/\",\n",
    "    json={\n",
    "        \"instances\": [\n",
    "            # use the first picture of the first batch\n",
    "            # make it a serializable list\n",
    "            image_to_predict.numpy().tolist()\n",
    "        ]\n",
    "    },\n",
    "    headers={\"accessToken\": access_token}\n",
    ")\n",
    "print(\"Response from server:\")\n",
    "pprint(response.json())\n",
    "print(\"Digit prediction: \", np.array(response.json()[\"predictions\"]).argmax(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "associate-manitoba",
   "metadata": {},
   "source": [
    "## Batch processing\n",
    "If we want, we can send multiple data samples for prediction. This is especially useful for batch processing workloads that might be running in the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "worldwide-adrian",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 Images were classified\n",
      "Accuracy: 0.96875\n"
     ]
    }
   ],
   "source": [
    "response = requests.post(\n",
    "        \"https://app.easytensor.com/query/\",\n",
    "        json={\n",
    "            \"instances\": [\n",
    "                # use the entireity of first batch\n",
    "                # make it a serializable list\n",
    "                image_batches[0].numpy().tolist()\n",
    "            ]\n",
    "        },\n",
    "        headers={\"accessToken\": access_token}\n",
    "    )\n",
    "predictions = np.array(response.json()[\"predictions\"])\n",
    "correct = 0\n",
    "for prediction, label in zip(\n",
    "    predictions.argmax(axis=1), label_batches[0]\n",
    "):\n",
    "    if prediction == label.numpy():\n",
    "        correct += 1\n",
    "print(\"{} Images were classified\".format(len(predictions)))\n",
    "print(\"Accuracy:\", correct/len(predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "still-effectiveness",
   "metadata": {},
   "source": [
    "# PyTorch Text Classifier\n",
    "We will create a text classifier that can differentiate between 4 news categories.\n",
    "This is adapted from [this tutorial](https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brown-sapphire",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torchtext torch-model-archiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "absolute-recruitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.datasets import AG_NEWS\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "train_iter = AG_NEWS(split='train')\n",
    "counter = Counter()\n",
    "for (label, line) in train_iter:\n",
    "    counter.update(tokenizer(line))\n",
    "vocab = Vocab(counter, min_freq=1)\n",
    "\n",
    "text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]\n",
    "label_pipeline = lambda x: int(x) - 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "still-chicago",
   "metadata": {},
   "source": [
    "### Generate data batch and iterator\n",
    "[torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader) is recommended for PyTorch users (a tutorial is [here](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html)). It works with a map-style dataset that implements the getitem() and len() protocols, and represents a map from indices/keys to data samples. It also works with an iterable datasets with the shuffle argumnent of False.\n",
    "\n",
    "Before sending to the model, `collate_fn` function works on a batch of samples generated from `DataLoader`. The input to `collate_fn` is a batch of data with the batch size in `DataLoader`, and `collate_fn` processes them according to the data processing pipelines declared previouly. Pay attention here and make sure that `collate_fn` is declared as a top level def. This ensures that the function is available in each worker.\n",
    "\n",
    "In this example, the text entries in the original data batch input are packed into a list and concatenated as a single tensor for the input of nn.EmbeddingBag. The offset is a tensor of delimiters to represent the beginning index of the individual sequence in the text tensor. Label is a tensor saving the labels of indidividual text entries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "continuing-generation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for (_label, _text) in batch:\n",
    "         label_list.append(label_pipeline(_label))\n",
    "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "         text_list.append(processed_text)\n",
    "         offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
    "\n",
    "train_iter = AG_NEWS(split='train')\n",
    "dataloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-sessions",
   "metadata": {},
   "source": [
    "The model is composed of the [nn.EmbeddingBag](https://pytorch.org/docs/stable/nn.html?highlight=embeddingbag#torch.nn.EmbeddingBag) layer plus a linear layer for the classification purpose. nn.EmbeddingBag with the default mode of “mean” computes the mean value of a “bag” of embeddings. Although the text entries here have different lengths, `nn.EmbeddingBag` module requires no padding here since the text lengths are saved in offsets.\n",
    "\n",
    "Additionally, since `nn.EmbeddingBag` accumulates the average across the embeddings on the fly, `nn.EmbeddingBag` can enhance the performance and memory efficiency to process a sequence of tensors.\n",
    "![img](img/text_sentiment_pytorch.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "executive-stephen",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size=95812, embed_dim=64, num_class=4):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-vessel",
   "metadata": {},
   "source": [
    "### Initiate an instance\n",
    "The `AG_NEWS` dataset has four labels and therefore the number of classes is four.\n",
    "\n",
    "1. World\n",
    "2. Sports\n",
    "3. Business\n",
    "4. Sci/Tec\n",
    "\n",
    "We build a model with the embedding dimension of 64. The vocab size is equal to the length of the vocabulary instance. The number of classes is equal to the number of labels,\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "following-sustainability",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 95812\n",
      "emsize: 64\n",
      "num_classes: 4\n"
     ]
    }
   ],
   "source": [
    "train_iter = AG_NEWS(split='train')\n",
    "num_class = len(set([label for (label, text) in train_iter]))\n",
    "vocab_size = len(vocab)\n",
    "emsize = 64\n",
    "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)\n",
    "print(\"vocab size:\", vocab_size)\n",
    "print(\"emsize:\", emsize)\n",
    "print(\"num_classes:\", num_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-growing",
   "metadata": {},
   "source": [
    "### Define functions to train the model and evaluate results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "necessary-foundation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predited_label = model(text, offsets)\n",
    "        loss = criterion(predited_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predited_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
    "                                              total_acc/total_count))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predited_label = model(text, offsets)\n",
    "            loss = criterion(predited_label, label)\n",
    "            total_acc += (predited_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc/total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thousand-falls",
   "metadata": {},
   "source": [
    "### Split the dataset and run the model\n",
    "Since the original `AG_NEWS` has no valid dataset, we split the training dataset into train/valid sets with a split ratio of 0.95 (train) and 0.05 (valid). Here we use [torch.utils.data.dataset.random_split](https://pytorch.org/docs/stable/data.html?highlight=random_split#torch.utils.data.random_split) function in PyTorch core library.\n",
    "\n",
    "[CrossEntropyLoss](https://pytorch.org/docs/stable/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss) criterion combines `nn.LogSoftmax()` and `nn.NLLLoss()` in a single class. It is useful when training a classification problem with C classes. [SGD](https://pytorch.org/docs/stable/_modules/torch/optim/sgd.html) implements stochastic gradient descent method as the optimizer. The initial learning rate is set to 5.0. [StepLR](https://pytorch.org/docs/master/_modules/torch/optim/lr_scheduler.html#StepLR) is used here to adjust the learning rate through epochs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "artificial-nashville",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   500/ 1782 batches | accuracy    0.681\n",
      "| epoch   1 |  1000/ 1782 batches | accuracy    0.851\n",
      "| epoch   1 |  1500/ 1782 batches | accuracy    0.879\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time: 14.97s | valid accuracy    0.888 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |   500/ 1782 batches | accuracy    0.897\n",
      "| epoch   2 |  1000/ 1782 batches | accuracy    0.902\n",
      "| epoch   2 |  1500/ 1782 batches | accuracy    0.902\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time: 13.54s | valid accuracy    0.893 \n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |   500/ 1782 batches | accuracy    0.915\n",
      "| epoch   3 |  1000/ 1782 batches | accuracy    0.914\n",
      "| epoch   3 |  1500/ 1782 batches | accuracy    0.915\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time: 13.11s | valid accuracy    0.897 \n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |   500/ 1782 batches | accuracy    0.924\n",
      "| epoch   4 |  1000/ 1782 batches | accuracy    0.925\n",
      "| epoch   4 |  1500/ 1782 batches | accuracy    0.922\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time: 15.64s | valid accuracy    0.898 \n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |   500/ 1782 batches | accuracy    0.931\n",
      "| epoch   5 |  1000/ 1782 batches | accuracy    0.931\n",
      "| epoch   5 |  1500/ 1782 batches | accuracy    0.928\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time: 14.85s | valid accuracy    0.910 \n",
      "-----------------------------------------------------------\n",
      "| epoch   6 |   500/ 1782 batches | accuracy    0.935\n",
      "| epoch   6 |  1000/ 1782 batches | accuracy    0.935\n",
      "| epoch   6 |  1500/ 1782 batches | accuracy    0.935\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time: 14.46s | valid accuracy    0.909 \n",
      "-----------------------------------------------------------\n",
      "| epoch   7 |   500/ 1782 batches | accuracy    0.948\n",
      "| epoch   7 |  1000/ 1782 batches | accuracy    0.945\n",
      "| epoch   7 |  1500/ 1782 batches | accuracy    0.948\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time: 12.98s | valid accuracy    0.910 \n",
      "-----------------------------------------------------------\n",
      "| epoch   8 |   500/ 1782 batches | accuracy    0.949\n",
      "| epoch   8 |  1000/ 1782 batches | accuracy    0.946\n",
      "| epoch   8 |  1500/ 1782 batches | accuracy    0.949\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time: 12.84s | valid accuracy    0.909 \n",
      "-----------------------------------------------------------\n",
      "| epoch   9 |   500/ 1782 batches | accuracy    0.951\n",
      "| epoch   9 |  1000/ 1782 batches | accuracy    0.951\n",
      "| epoch   9 |  1500/ 1782 batches | accuracy    0.949\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time: 12.83s | valid accuracy    0.909 \n",
      "-----------------------------------------------------------\n",
      "| epoch  10 |   500/ 1782 batches | accuracy    0.950\n",
      "| epoch  10 |  1000/ 1782 batches | accuracy    0.950\n",
      "| epoch  10 |  1500/ 1782 batches | accuracy    0.950\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time: 12.75s | valid accuracy    0.909 \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "# Hyperparameters\n",
    "EPOCHS = 10 # epoch\n",
    "LR = 5  # learning rate\n",
    "BATCH_SIZE = 64 # batch size for training\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "total_accu = None\n",
    "train_iter, test_iter = AG_NEWS()\n",
    "train_dataset = list(train_iter)\n",
    "test_dataset = list(test_iter)\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = \\\n",
    "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "\n",
    "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                             shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader)\n",
    "    accu_val = evaluate(valid_dataloader)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "      scheduler.step()\n",
    "    else:\n",
    "       total_accu = accu_val\n",
    "    print('-' * 59)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "          'valid accuracy {:8.3f} '.format(epoch,\n",
    "                                           time.time() - epoch_start_time,\n",
    "                                           accu_val))\n",
    "    print('-' * 59)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-profit",
   "metadata": {},
   "source": [
    "### Evaluate the model with test dataset\n",
    "Checking the results of the test dataset…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "strange-blackjack",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the results of test dataset.\n",
      "test accuracy    0.906\n"
     ]
    }
   ],
   "source": [
    "print('Checking the results of test dataset.')\n",
    "accu_test = evaluate(test_dataloader)\n",
    "print('test accuracy {:8.3f}'.format(accu_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "round-baseline",
   "metadata": {},
   "source": [
    "Test on a random news\n",
    "Use the best model so far and test a golf news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "gorgeous-referral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a Sports news\n"
     ]
    }
   ],
   "source": [
    "ag_news_label = {1: \"World\",\n",
    "                 2: \"Sports\",\n",
    "                 3: \"Business\",\n",
    "                 4: \"Sci/Tec\"}\n",
    "\n",
    "def predict(text, text_pipeline):\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor(text_pipeline(text))\n",
    "        output = model(text, torch.tensor([0]))\n",
    "        return output.argmax(1).item() + 1\n",
    "\n",
    "ex_text_str = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n",
    "    enduring the season’s worst weather conditions on Sunday at The \\\n",
    "    Open on his way to a closing 75 at Royal Portrush, which \\\n",
    "    considering the wind and the rain was a respectable showing. \\\n",
    "    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n",
    "    was another story. With temperatures in the mid-80s and hardly any \\\n",
    "    wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
    "    Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
    "    finished with an 8-under 62 for a three-stroke lead, which \\\n",
    "    was even more impressive considering he’d never played the \\\n",
    "    front nine at TPC Southwind.\"\n",
    "\n",
    "model = model.to(\"cpu\")\n",
    "\n",
    "print(\"This is a %s news\" %ag_news_label[predict(ex_text_str, text_pipeline)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interim-static",
   "metadata": {},
   "source": [
    "## Save the model\n",
    "A PyTorch model requires slightly more pieces of information than a TensorFlow model to run correctly.\n",
    "1. The serialized parameters (weights and biases) of the model. EasyTensor takes care of that for you.\n",
    "2. The model class, including a prediction method. **You have to do this.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pregnant-kazakhstan",
   "metadata": {},
   "source": [
    "#### Saving Model Class\n",
    "For the model class, we save the model definition in a class in a separate file. \n",
    "\n",
    "This model class must include a `predict_single` method that can take in your native input format (e.g. text, image bytes, number array) and return a human readible output. You can think of the `predict_single` method as where the preprocess, predict, and postprocess happens.\n",
    "\n",
    "Here is a simple file that contains a more complete version of our class definition above. Note how `single_predict` takes in a text format (`str`) and returns a string representation of the predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "olympic-attention",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = \"\"\"\n",
    "from torch import nn\n",
    "import torch.tensor\n",
    "from torchtext.datasets import AG_NEWS\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "train_iter = AG_NEWS(split=\"train\")\n",
    "counter = Counter()\n",
    "for (label, line) in train_iter:\n",
    "    counter.update(tokenizer(line))\n",
    "vocab = Vocab(counter, min_freq=1)\n",
    "\n",
    "text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]\n",
    "label_pipeline = lambda x: int(x) - 1\n",
    "\n",
    "ag_news_label = {1: \"World\", 2: \"Sports\", 3: \"Business\", 4: \"Sci/Tec\"}\n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size=95812, embed_dim=64, num_class=4):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "        self.tokenizer = text_pipeline\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, tokens, offset):\n",
    "        embedded = self.embedding(tokens, offset)\n",
    "        return self.fc(embedded)\n",
    "\n",
    "    def predict_single(self, input: str):\n",
    "        tokenized = torch.tensor(text_pipeline(input))\n",
    "        output = self.forward(tokenized, torch.tensor([0]))\n",
    "        prediction = output.argmax(1).item() + 1\n",
    "        return ag_news_label[prediction]\n",
    "\n",
    "\"\"\"\n",
    "with open(\"model_file.py\", \"w\") as fout:\n",
    "    fout.write(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "packed-handy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "from torch import nn\r\n",
      "import torch.tensor\r\n",
      "from torchtext.datasets import AG_NEWS\r\n",
      "\r\n",
      "from torchtext.data.utils import get_tokenizer\r\n",
      "from collections import Counter\r\n",
      "from torchtext.vocab import Vocab\r\n",
      "\r\n",
      "tokenizer = get_tokenizer(\"basic_english\")\r\n",
      "train_iter = AG_NEWS(split=\"train\")\r\n",
      "counter = Counter()\r\n",
      "for (label, line) in train_iter:\r\n",
      "    counter.update(tokenizer(line))\r\n",
      "vocab = Vocab(counter, min_freq=1)\r\n",
      "\r\n",
      "text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]\r\n",
      "label_pipeline = lambda x: int(x) - 1\r\n",
      "\r\n",
      "ag_news_label = {1: \"World\", 2: \"Sports\", 3: \"Business\", 4: \"Sci/Tec\"}\r\n",
      "\r\n",
      "class TextClassificationModel(nn.Module):\r\n",
      "\r\n",
      "    def __init__(self, vocab_size=95812, embed_dim=64, num_class=4):\r\n",
      "        super(TextClassificationModel, self).__init__()\r\n",
      "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\r\n",
      "        self.fc = nn.Linear(embed_dim, num_class)\r\n",
      "        self.init_weights()\r\n",
      "        self.tokenizer = text_pipeline\r\n",
      "\r\n",
      "    def init_weights(self):\r\n",
      "        initrange = 0.5\r",
      "\r\n",
      "        self.embedding.weight.data.uniform_(-initrange, initrange)\r\n",
      "        self.fc.weight.data.uniform_(-initrange, initrange)\r\n",
      "        self.fc.bias.data.zero_()\r\n",
      "\r\n",
      "    def forward(self, tokens, offset):\r\n",
      "        embedded = self.embedding(tokens, offset)\r\n",
      "        return self.fc(embedded)\r\n",
      "\r\n",
      "    def predict_single(self, input: str):\r\n",
      "        tokenized = torch.tensor(text_pipeline(input))\r\n",
      "        output = self.forward(tokenized, torch.tensor([0]))\r\n",
      "        prediction = output.argmax(1).item() + 1\r\n",
      "        return ag_news_label[prediction]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! cat model_file.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removable-invalid",
   "metadata": {},
   "source": [
    "## Upload the model to EasyTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secondary-hopkins",
   "metadata": {},
   "source": [
    "At this piont, we have all we need to upload our pytorch model to easytensor. Point easytensor to your parameter file and the model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pending-glance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import easytensor\n",
    "easytensor.pytorch.upload_model(\"test_pt_final\", model, \"model_file.py\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
